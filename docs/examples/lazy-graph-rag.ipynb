{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Retrieval Beyond Similarity: Lazy GraphRAG in LangChain\"\n",
    "author: Ben Chambers\n",
    "execute:\n",
    "    output: false\n",
    "    warning: false\n",
    "    error: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gqhMgAGwmLXc"
   },
   "source": [
    "# Retrieval Beyond Similarity: LazyGraphRAG in LangChain\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In [LazyGraphRAG](https://www.microsoft.com/en-us/research/blog/lazygraphrag-setting-a-new-standard-for-quality-and-cost/), Microsoft demonstrates significant cost and performance benefits to delaying the construction of a knowledge graph.\n",
    "This is largely because not all documents need to be analyzed.\n",
    "However, it is also benefical that documents by the time documents are analyzed the question is already known, allowing irrelevant information to be ignored. \n",
    "\n",
    "We've noticed similar cost benefits to building a document graph linking content based on simple properties such as extracted keywords compared to building a complete knowledge graph.\n",
    "For the Wikipedia dataset used in this notebook, we estimated it would have taken $70k to build a knowledege graph using the [example from LangChain](https://python.langchain.com/docs/how_to/graph_constructing/#llm-graph-transformer), while the document graph was basically free.\n",
    "\n",
    "In this notebook we demonstrate how to populate a document graph with Wikipedia articles linked based on mentions in the articles and extracted keywords.\n",
    "Keyword extraction uses a local [KeyBERT](https://maartengr.github.io/KeyBERT/) model, making it fast and cost-effective to construct these graphs.\n",
    "We'll then show how to build out a chain which does the steps of Lazy GraphRAG -- retrieving articles, extracting claims from each community, ranking and selecting the top claims, and generating an answer based on those claims."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k6dFtwI_xmFW"
   },
   "source": [
    "## Environment Setup\n",
    "\n",
    "The following block will configure the environment from the Colab Secrets.\n",
    "To run it, you should have the following Colab Secrets defined and accessible to this notebook:\n",
    "\n",
    "- `OPENAI_API_KEY`: The OpenAI key.\n",
    "- `ASTRA_DB_API_ENDPOINT`: The Astra DB API endpoint.\n",
    "- `ASTRA_DB_APPLICATION_TOKEN`: The Astra DB Application token.\n",
    "- `LANGCHAIN_API_KEY`: Optional. If defined, will enable LangSmith tracing.\n",
    "- `ASTRA_DB_KEYSPACE`: Optional. If defined, will specify the Astra DB keyspace. If not defined, will use the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "r0-5VJWGsBM3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-core in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (0.3.31)\n",
      "Requirement already satisfied: langchain-astradb in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (0.5.2)\n",
      "Requirement already satisfied: langchain-openai in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (0.3.1)\n",
      "Requirement already satisfied: langchain-graph-retriever in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (0.1.0)\n",
      "Requirement already satisfied: graph-rag-example-helpers in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (0.1.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from langchain-core) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from langchain-core) (0.2.10)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from langchain-core) (24.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from langchain-core) (2.10.4)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from langchain-core) (9.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from langchain-core) (4.12.2)\n",
      "Requirement already satisfied: astrapy<2.0.0,>=1.5.2 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from langchain-astradb) (1.5.2)\n",
      "Requirement already satisfied: langchain-community>=0.3.1 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from langchain-astradb) (0.3.14)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from langchain-astradb) (1.26.4)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from langchain-openai) (1.60.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from langchain-openai) (0.8.0)\n",
      "Requirement already satisfied: networkx>=3.4.2 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from langchain-graph-retriever) (3.4.2)\n",
      "Requirement already satisfied: backoff>=2.2.1 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from graph-rag-example-helpers) (2.2.1)\n",
      "Requirement already satisfied: httpx>=0.28.1 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from graph-rag-example-helpers) (0.28.1)\n",
      "Requirement already satisfied: python-dotenv>=1.0.1 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from graph-rag-example-helpers) (1.0.1)\n",
      "Requirement already satisfied: simsimd>=6.2.1 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from graph-rag-example-helpers) (6.2.1)\n",
      "Requirement already satisfied: tqdm>=4.67.1 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from graph-rag-example-helpers) (4.67.1)\n",
      "Requirement already satisfied: deprecation<2.2.0,>=2.1.0 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from astrapy<2.0.0,>=1.5.2->langchain-astradb) (2.1.0)\n",
      "Requirement already satisfied: pymongo>=3 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from astrapy<2.0.0,>=1.5.2->langchain-astradb) (4.10.1)\n",
      "Requirement already satisfied: toml<0.11.0,>=0.10.2 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from astrapy<2.0.0,>=1.5.2->langchain-astradb) (0.10.2)\n",
      "Requirement already satisfied: uuid6>=2024.1.12 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from astrapy<2.0.0,>=1.5.2->langchain-astradb) (2024.7.10)\n",
      "Requirement already satisfied: anyio in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from httpx>=0.28.1->graph-rag-example-helpers) (4.8.0)\n",
      "Requirement already satisfied: certifi in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from httpx>=0.28.1->graph-rag-example-helpers) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from httpx>=0.28.1->graph-rag-example-helpers) (1.0.7)\n",
      "Requirement already satisfied: idna in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from httpx>=0.28.1->graph-rag-example-helpers) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.28.1->graph-rag-example-helpers) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from langchain-community>=0.3.1->langchain-astradb) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from langchain-community>=0.3.1->langchain-astradb) (3.11.11)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from langchain-community>=0.3.1->langchain-astradb) (0.6.7)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from langchain-community>=0.3.1->langchain-astradb) (0.4.0)\n",
      "Requirement already satisfied: langchain<0.4.0,>=0.3.14 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from langchain-community>=0.3.1->langchain-astradb) (0.3.14)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from langchain-community>=0.3.1->langchain-astradb) (2.7.1)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from langchain-community>=0.3.1->langchain-astradb) (2.32.3)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain-core) (3.10.13)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain-core) (1.0.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.8.2)\n",
      "Requirement already satisfied: sniffio in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (2.27.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.3.1->langchain-astradb) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.3.1->langchain-astradb) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.3.1->langchain-astradb) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.3.1->langchain-astradb) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.3.1->langchain-astradb) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.3.1->langchain-astradb) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.3.1->langchain-astradb) (1.18.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community>=0.3.1->langchain-astradb) (3.24.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community>=0.3.1->langchain-astradb) (0.9.0)\n",
      "Requirement already satisfied: h2<5,>=3 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from httpx[http2]<1,>=0.25.2->astrapy<2.0.0,>=1.5.2->langchain-astradb) (4.1.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from langchain<0.4.0,>=0.3.14->langchain-community>=0.3.1->langchain-astradb) (0.3.5)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from pymongo>=3->astrapy<2.0.0,>=1.5.2->langchain-astradb) (2.7.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from requests<3,>=2->langchain-community>=0.3.1->langchain-astradb) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from requests<3,>=2->langchain-community>=0.3.1->langchain-astradb) (2.3.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from h2<5,>=3->httpx[http2]<1,>=0.25.2->astrapy<2.0.0,>=1.5.2->langchain-astradb) (6.0.1)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from h2<5,>=3->httpx[http2]<1,>=0.25.2->astrapy<2.0.0,>=1.5.2->langchain-astradb) (4.0.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community>=0.3.1->langchain-astradb) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#@ Install modules.\n",
    "%pip install \\\n",
    "    langchain-core \\\n",
    "    langchain-astradb \\\n",
    "    langchain-openai \\\n",
    "    langchain-graph-retriever \\\n",
    "    graph-rag-example-helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last package -- `graph-rag-example-helpers` -- includes some helpers for setting up environment helpers and allowing the loading of wikipedia data to be restarted if it fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure import paths.\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "# Initialize environment variables.\n",
    "from graph_rag_example_helpers.env import Environment, initialize_environment\n",
    "\n",
    "initialize_environment(Environment.ASTRAPY)\n",
    "\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"lazy-graph-rag\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Loading Data\n",
    "\n",
    "First, we'll demonstrate how to load Wikipedia data into an `AstraDBVectorStore`, using the mentioned articles and keywords as metadata fields.\n",
    "In this section, we're not actually doing anything special for the graph -- we're just populating the metadata with fields that useful describe our content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Documents from Wikipedia Articles\n",
    "The first thing we need to do is create the `LangChain` `Document`s we'll import.\n",
    "\n",
    "To do this, we write some code to convert lines from a JSON file downloaded from [2wikimultihop](https://github.com/Alab-NII/2wikimultihop?tab=readme-ov-file#new-update-april-7-2021) and create a `Document`.\n",
    "We populate the `id` and `metadata[\"mentions\"]` from information in this file.\n",
    "\n",
    "Then, we run those documents through the `KeybertKeywordExtractor` to populate `metadata[\"keywords\"]` with the suggested keywords from each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8u4lD-AqDMMs"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections.abc import Iterator\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_graph_retriever.document_transformers.keybert import KeybertKeywordExtractor\n",
    "\n",
    "def parse_document(line: bytes) -> Document:\n",
    "    \"\"\"Reads one JSON line from the wikimultihop dump.\"\"\"\n",
    "    para = json.loads(line)\n",
    "\n",
    "    id = para[\"id\"]\n",
    "\n",
    "    # Use structured information (mentioned Wikipedia IDs) as metadata.\n",
    "    mentioned_ids = [id for m in para[\"mentions\"] for m in m[\"ref_ids\"] or []]\n",
    "\n",
    "    return Document(\n",
    "        id=id,\n",
    "        page_content=\" \".join(para[\"sentences\"]),\n",
    "        metadata={\n",
    "            \"mentions\": mentioned_ids,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "KEYBERT_TRANSFORMER = KeybertKeywordExtractor()\n",
    "\n",
    "\n",
    "# Load data in batches, using GLiNER to extract entities.\n",
    "def prepare_batch(lines: Iterator[str]) -> Iterator[Document]:\n",
    "    # Parse documents from the batch of lines.\n",
    "    docs = [parse_document(line) for line in lines]\n",
    "\n",
    "    docs = KEYBERT_TRANSFORMER.transform_documents(docs)\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the AstraDBVectorStore\n",
    "Next, we create the Vector Store we're going to load these documents into.\n",
    "In our case, we use DataStax Astra DB with Open AI embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ Create the AstraDBVectorStore\n",
    "\n",
    "from langchain_astradb import AstraDBVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "COLLECTION = \"lazy_graph_rag\"\n",
    "store = AstraDBVectorStore(\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    collection_name=COLLECTION,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data into the Store\n",
    "Next, we perform the actual loading.\n",
    "This takes a while, so we use a helper utility to persist which batches have been written so we can resume if there are any failures.\n",
    "\n",
    "On OS X, it is useful to run `caffeinate -dis` in a shell to prevent the machine from going to sleep and seems to reduce errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming loading with 13 completed, 5977 remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 33/5977 [36:11<108:38:24, 65.80s/it]\n",
      "  + Exception Group Traceback (most recent call last):\n",
      "  |   File \"/Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
      "  |     await eval(code_obj, self.user_global_ns, self.user_ns)\n",
      "  |   File \"/var/folders/c4/dcr0mh3d183d5kh9gf89wsc00000gn/T/ipykernel_60249/1917728629.py\", line 12, in <module>\n",
      "  |     await aload_2wikimultihop(\n",
      "  |   File \"/Users/benjamin.chambers/code/graph-pancake/packages/graph-rag-example-helpers/src/graph_rag_example_helpers/datasets/wikimultihop/load.py\", line 86, in aload_2wikimultihop\n",
      "  |     async with asyncio.TaskGroup() as tg:\n",
      "  |                ^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/Users/benjamin.chambers/.local/share/uv/python/cpython-3.12.8-macos-aarch64-none/lib/python3.12/asyncio/taskgroups.py\", line 71, in __aexit__\n",
      "  |     return await self._aexit(et, exc)\n",
      "  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/Users/benjamin.chambers/.local/share/uv/python/cpython-3.12.8-macos-aarch64-none/lib/python3.12/asyncio/taskgroups.py\", line 164, in _aexit\n",
      "  |     raise BaseExceptionGroup(\n",
      "  | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n",
      "  +-+---------------- 1 ----------------\n",
      "    | Traceback (most recent call last):\n",
      "    |   File \"/Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n",
      "    |     yield\n",
      "    |   File \"/Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages/httpx/_transports/default.py\", line 394, in handle_async_request\n",
      "    |     resp = await self._pool.handle_async_request(req)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py\", line 256, in handle_async_request\n",
      "    |     raise exc from None\n",
      "    |   File \"/Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py\", line 236, in handle_async_request\n",
      "    |     response = await connection.handle_async_request(\n",
      "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py\", line 103, in handle_async_request\n",
      "    |     return await self._connection.handle_async_request(request)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py\", line 136, in handle_async_request\n",
      "    |     raise exc\n",
      "    |   File \"/Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py\", line 106, in handle_async_request\n",
      "    |     ) = await self._receive_response_headers(**kwargs)\n",
      "    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py\", line 177, in _receive_response_headers\n",
      "    |     event = await self._receive_event(timeout=timeout)\n",
      "    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py\", line 231, in _receive_event\n",
      "    |     raise RemoteProtocolError(msg)\n",
      "    | httpcore.RemoteProtocolError: Server disconnected without sending a response.\n",
      "    | \n",
      "    | The above exception was the direct cause of the following exception:\n",
      "    | \n",
      "    | Traceback (most recent call last):\n",
      "    |   File \"/Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages/backoff/_async.py\", line 151, in retry\n",
      "    |     ret = await target(*args, **kwargs)\n",
      "    |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/benjamin.chambers/code/graph-pancake/packages/graph-rag-example-helpers/src/graph_rag_example_helpers/datasets/wikimultihop/load.py\", line 95, in add_docs\n",
      "    |     await store.aadd_documents(batch_docs)\n",
      "    |   File \"/Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages/langchain_core/vectorstores/base.py\", line 322, in aadd_documents\n",
      "    |     return await self.aadd_texts(texts, metadatas, **kwargs)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages/langchain_astradb/vectorstores.py\", line 1224, in aadd_texts\n",
      "    |     replace_results = await asyncio.gather(*tasks, return_exceptions=False)\n",
      "    |                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages/langchain_astradb/vectorstores.py\", line 1214, in _replace_document\n",
      "    |     return await _async_collection.replace_one(\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages/astrapy/collection.py\", line 4482, in replace_one\n",
      "    |     fo_response = await self._api_commander.async_request(\n",
      "    |                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages/astrapy/api_commander.py\", line 354, in async_request\n",
      "    |     raw_response = await self.async_raw_request(\n",
      "    |                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages/astrapy/api_commander.py\", line 301, in async_raw_request\n",
      "    |     raw_response = await self.async_client.request(\n",
      "    |                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 1540, in request\n",
      "    |     return await self.send(request, auth=auth, follow_redirects=follow_redirects)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 1629, in send\n",
      "    |     response = await self._send_handling_auth(\n",
      "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 1657, in _send_handling_auth\n",
      "    |     response = await self._send_handling_redirects(\n",
      "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 1694, in _send_handling_redirects\n",
      "    |     response = await self._send_single_request(request)\n",
      "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 1730, in _send_single_request\n",
      "    |     response = await transport.handle_async_request(request)\n",
      "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages/httpx/_transports/default.py\", line 393, in handle_async_request\n",
      "    |     with map_httpcore_exceptions():\n",
      "    |          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/benjamin.chambers/.local/share/uv/python/cpython-3.12.8-macos-aarch64-none/lib/python3.12/contextlib.py\", line 158, in __exit__\n",
      "    |     self.gen.throw(value)\n",
      "    |   File \"/Users/benjamin.chambers/code/graph-pancake/.venv/lib/python3.12/site-packages/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n",
      "    |     raise mapped_exc(message) from exc\n",
      "    | httpx.RemoteProtocolError: Server disconnected without sending a response.\n",
      "    +------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path\n",
    "\n",
    "# Path to the file `para_with_hyperlink.zip`.\n",
    "# See instructions here to download from\n",
    "# [2wikimultihop](https://github.com/Alab-NII/2wikimultihop?tab=readme-ov-file#new-update-april-7-2021).\n",
    "PARA_WITH_HYPERLINK_ZIP = os.path.join(\n",
    "    os.getcwd(), \"para_with_hyperlink.zip\"\n",
    ")\n",
    "\n",
    "from graph_rag_example_helpers.datasets.wikimultihop import aload_2wikimultihop\n",
    "await aload_2wikimultihop(\n",
    "    PARA_WITH_HYPERLINK_ZIP,\n",
    "    store,\n",
    "    prepare_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we've created a `VectorStore` with the Wikipedia articles.\n",
    "Each article is associated with metadata identifying other articles it mentions and keywords from the article.\n",
    "This could be used for hybrid search -- performing a vector search for articles similar to a specific question *that also mention a specific term*.\n",
    "\n",
    "The library `langchain-graph-retriever` makes this even more useful, allowing traversing between articles either explicitly mentioned or dealing with the same keywords.\n",
    "\n",
    "In the next section, we'll go a step further and perform Lazy GraphRAG to extract relevant claims from both the similar and related articles and use the most relevant claims to answer the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Lazy Graph RAG via Hierarchical Summarization\n",
    "\n",
    "As we've noted before, eagerly building a knowledge graph is prohibitively expensive.\n",
    "Microsoft seems to agree, and recently introduced LazyGraphRAG, which enables GraphRAG to be performed late -- after a query is retrieved.\n",
    "\n",
    "We implement the LazyGraphRAG technique using the traversing retrievers as follows:\n",
    "\n",
    "1. Retrieve a good number of nodes using a traversing retrieval.\n",
    "2. Identify communities in the retrieved sub-graph.\n",
    "3. Extract claims from each community relevant to the query using an LLM.\n",
    "4. Rank each of the claims based on the relevance to the question and select the top claims.\n",
    "5. Generate an answer to the question based on the extracted claims."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain for Extracting Claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Iterable\n",
    "from operator import itemgetter\n",
    "from typing import TypedDict\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnableParallel, chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Claim(BaseModel):\n",
    "    \"\"\"Representation of an individual claim from a source document(s).\"\"\"\n",
    "\n",
    "    claim: str = Field(description=\"The claim from the original document(s).\")\n",
    "    source_id: str = Field(description=\"Document ID containing the claim.\")\n",
    "\n",
    "\n",
    "class Claims(BaseModel):\n",
    "    \"\"\"Claims extracted from a set of source document(s).\"\"\"\n",
    "\n",
    "    claims: list[Claim] = Field(description=\"The extracted claims.\")\n",
    "\n",
    "\n",
    "MODEL = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "CLAIMS_MODEL = MODEL.with_structured_output(Claims)\n",
    "\n",
    "CLAIMS_PROMPT = ChatPromptTemplate.from_template(\"\"\"\n",
    "Extract claims from the following related documents.\n",
    "\n",
    "Only return claims appearing within the specified documents.\n",
    "If no documents are provided, do not make up claims or documents.\n",
    "\n",
    "Claims (and scores) should be relevant to the question.\n",
    "Don't include claims from the documents if they are not directly or indirectly\n",
    "relevant to the question.\n",
    "\n",
    "If none of the documents make any claims relevant to the question, return an\n",
    "empty list of claims.\n",
    "\n",
    "If multiple documents make similar claims, include the original text of each as\n",
    "separate claims. Score the most useful and authoritative claim higher than\n",
    "similar, lower-quality claims.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "{formatted_documents}\n",
    "\"\"\")\n",
    "\n",
    "# TODO: Few-shot examples? Possibly with a selector?\n",
    "\n",
    "def format_documents_with_ids(documents: Iterable[Document]) -> str:\n",
    "    formatted_docs = \"\\n\\n\".join(\n",
    "        f\"Document ID: {doc.id}\\nContent: {doc.page_content}\" for doc in documents\n",
    "    )\n",
    "    return formatted_docs\n",
    "\n",
    "\n",
    "CLAIM_CHAIN = (\n",
    "    RunnableParallel(\n",
    "        {\n",
    "            \"question\": itemgetter(\"question\"),\n",
    "            \"formatted_documents\": itemgetter(\"documents\")\n",
    "            | RunnableLambda(format_documents_with_ids),\n",
    "        }\n",
    "    )\n",
    "    | CLAIMS_PROMPT\n",
    "    | CLAIMS_MODEL\n",
    ")\n",
    "\n",
    "\n",
    "class ClaimsChainInput(TypedDict):\n",
    "    question: str\n",
    "    communities: Iterable[Iterable[Document]]\n",
    "\n",
    "\n",
    "@chain\n",
    "async def claims_chain(input: ClaimsChainInput) -> Iterable[Claim]:\n",
    "    question = input[\"question\"]\n",
    "    communities = input[\"communities\"]\n",
    "\n",
    "    # TODO: Use openai directly so this can use the batch API for performance/cost?\n",
    "    community_claims = await CLAIM_CHAIN.abatch(\n",
    "        [{\"question\": question, \"documents\": community} for community in communities]\n",
    "    )\n",
    "    return [claim for community in community_claims for claim in community.claims]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain for Ranking Claims\n",
    "\n",
    "This is based on ideas from [RankRAG](https://arxiv.org/abs/2407.02485).\n",
    "Specifically, the prompt is constructed so that the next token should be `True` if the content is relevant and `False` if not.\n",
    "The probability of the token is used to determine the relevance -- `True` with a higher probability is more relevant than `True` with a lesser probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "RANK_PROMPT = ChatPromptTemplate.from_template(\"\"\"\n",
    "Rank the relevance of the following claim to the question.\n",
    "Output \"True\" if the claim is relevant and \"False\" if it is not.\n",
    "Only output True or False.\n",
    "\n",
    "Question: Where is Seattle?\n",
    "\n",
    "Claim: Seattle is in Washington State.\n",
    "\n",
    "Relevant: True\n",
    "\n",
    "Question: Where is LA?\n",
    "\n",
    "Claim: New York City is in New York State.\n",
    "\n",
    "Relevant: False\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Claim: {claim}\n",
    "\n",
    "Relevant:\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "def compute_rank(msg):\n",
    "    logprob = msg.response_metadata[\"logprobs\"][\"content\"][0]\n",
    "    prob = math.exp(logprob[\"logprob\"])\n",
    "    token = logprob[\"token\"]\n",
    "    if token == \"True\":\n",
    "        return prob\n",
    "    elif token == \"False\":\n",
    "        return 1.0 - prob\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected logprob: {logprob}\")\n",
    "\n",
    "\n",
    "RANK_CHAIN = RANK_PROMPT | MODEL.bind(logprobs=True) | RunnableLambda(compute_rank)\n",
    "\n",
    "\n",
    "class RankChainInput(TypedDict):\n",
    "    question: str\n",
    "    claims: Iterable[Claim]\n",
    "\n",
    "\n",
    "@chain\n",
    "async def rank_chain(input: RankChainInput) -> Iterable[Claim]:\n",
    "    # TODO: Use openai directly so this can use the batch API for performance/cost?\n",
    "    claims = input[\"claims\"]\n",
    "    ranks = await RANK_CHAIN.abatch(\n",
    "        [{\"question\": input[\"question\"], \"claim\": claim} for claim in claims]\n",
    "    )\n",
    "    rank_claims = sorted(\n",
    "        zip(ranks, claims, strict=True), key=lambda rank_claim: rank_claim[0]\n",
    "    )\n",
    "\n",
    "    return [claim for _, claim in rank_claims]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LazyGraphRAG in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.language_models import BaseLanguageModel\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.runnables import chain\n",
    "from langchain_graph_retriever.document_graphs import create_graph, group_by_community\n",
    "\n",
    "\n",
    "@chain\n",
    "async def lazy_graph_rag(\n",
    "    question: str,\n",
    "    *,\n",
    "    retriever: BaseRetriever,\n",
    "    edges: Iterable[str | tuple[str, str]] | None = None,\n",
    "    model: BaseLanguageModel,\n",
    "    max_tokens: int = 1000,\n",
    ") -> str:\n",
    "    \"\"\"Retrieve claims relating to the question using LazyGraphRAG.\n",
    "\n",
    "    Returns the top claims up to the given `max_tokens` as a markdown list.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if edges is None:\n",
    "        try:\n",
    "            edges = retriever.edges\n",
    "        except AttributeError as _:\n",
    "            raise ValueError(\n",
    "                \"Must specify 'edges' or provide a retriever with 'edges' field defined\"\n",
    "            )\n",
    "\n",
    "    # 1. Retrieve documents using the (traversing) retriever.\n",
    "    documents = await retriever.ainvoke(question)\n",
    "\n",
    "    # 2. Create a graph and extract communities.\n",
    "    documents_by_id, doc_graph = create_graph(\n",
    "        documents,\n",
    "        edges=edges,\n",
    "        directed=False,\n",
    "    )\n",
    "    communities = group_by_community(documents_by_id, doc_graph)\n",
    "\n",
    "    # 3. Extract claims from the communities.\n",
    "    claims = await claims_chain.ainvoke(\n",
    "        {\"question\": question, \"communities\": communities}\n",
    "    )\n",
    "\n",
    "    # 4. Rank the claims and select claims up to the given token limit.\n",
    "    result_claims = []\n",
    "    tokens = 0\n",
    "\n",
    "    for claim in await rank_chain.ainvoke({\"question\": question, \"claims\": claims}):\n",
    "        claim_str = f\"- {claim.claim} (Source: {claim.source_id})\"\n",
    "\n",
    "        tokens += model.get_num_tokens(claim_str)\n",
    "        if tokens > max_tokens:\n",
    "            break\n",
    "        result_claims.append(claim_str)\n",
    "\n",
    "    return \"\\n\".join(result_claims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LazyGraphRAG in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers.graph_traversal import (\n",
    "    AstraTraversalAdapter,\n",
    "    GraphTraversalRetriever,\n",
    ")\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "EDGES = [(\"mentions\", \"id\"), \"entities\"]\n",
    "\n",
    "RETRIEVER = GraphTraversalRetriever(\n",
    "    store=AstraTraversalAdapter(store),\n",
    "    edges=EDGES,\n",
    "    start_k=100,\n",
    "    depth=3,\n",
    ")\n",
    "\n",
    "# RETRIEVER = GraphMMRTraversalRetriever(\n",
    "#     store = AstraMMRTraversalAdapter(store),\n",
    "#     edges = EDGES,\n",
    "#     k = 100,\n",
    "#     depth = 5,\n",
    "#     fetch_k = 100,\n",
    "#     adjacent_k = 25,\n",
    "#     lambda_mult = 0.8,\n",
    "#     score_threshold = float(\"-inf\"),\n",
    "# )\n",
    "\n",
    "ANSWER_PROMPT = PromptTemplate.from_template(\"\"\"\n",
    "Answer the question based on the supporting claims.\n",
    "\n",
    "Only use information from the claims. Do not guess or make up any information.\n",
    "\n",
    "Where possible, reference and quote the supporting claims.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Claims:\n",
    "{claims}\n",
    "\"\"\")\n",
    "\n",
    "LAZY_GRAPH_RAG_CHAIN = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"claims\": RunnablePassthrough()\n",
    "        | lazy_graph_rag.bind(\n",
    "            retriever=RETRIEVER,\n",
    "            model=MODEL,\n",
    "            max_tokens=1000,\n",
    "        ),\n",
    "    }\n",
    "    | ANSWER_PROMPT\n",
    "    | MODEL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await LAZY_GRAPH_RAG_CHAIN.ainvoke(\"Where is Azerbaijan?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LazyGraphRAG chain is great when a question needs to consider a large amount of relevant information in order to produce a thorough answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This post introduced _traversing retrievers_ which allow any `VectorStore` to be traversed as a knowledge graph based on properties in the metadata.\n",
    "This means you can focus on populating and using your `VectorStore` with useful metadata and add GraphRAG when you need it.\n",
    "We also saw that these traversing retrievers mean that any `VectorStore` can be used with LazyGraphRAG, without needing to change the stored documents.\n",
    "\n",
    "Knowledge Graphs and GraphRAG shouldn't be hard or scary.\n",
    "Start simple and easily overlay edges when you need them.\n",
    "\n",
    "These traversing retrievers and LazyGraphRAG summarization work well with agents.\n",
    "You can create tools that use different retriever configurations, for instance, searching for articles \"near\" existing articles or distinguishing between questions that only need a few references and deeper questions which need to retrieve and summarize a larger amount of content.\n",
    "We'll show how to combine these graph techniques with agents in future posts.\n",
    "Until then, ..."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
