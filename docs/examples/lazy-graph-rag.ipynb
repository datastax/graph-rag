{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqhMgAGwmLXc"
      },
      "source": [
        "# Retrieval Beyond Similarity: LazyGraphRAG in LangChain\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In [LazyGraphRAG](https://www.microsoft.com/en-us/research/blog/lazygraphrag-setting-a-new-standard-for-quality-and-cost/), Microsoft demonstrates significant cost and performance benefits to delaying the construction of a knowledge graph.\n",
        "This is largely because not all documents need to be analyzed.\n",
        "However, it is also benefical that documents by the time documents are analyzed the question is already known, allowing irrelevant information to be ignored. \n",
        "\n",
        "We've noticed similar cost benefits to building a document graph linking content based on simple properties such as extracted keywords compared to building a complete knowledge graph.\n",
        "For the Wikipedia dataset used in this notebook, we estimated it would have taken $70k to build a knowledege graph using the [example from LangChain](https://python.langchain.com/docs/how_to/graph_constructing/#llm-graph-transformer), while the document graph was basically free.\n",
        "\n",
        "These approaches are largely compatible.\n",
        "In this notebook, we demonstrate how to populate a document graph with Wikipedia articles linked based on mentions in the articles and extracted keywords.\n",
        "Keyword extraction uses a local [KeyBERT](https://maartengr.github.io/KeyBERT/) model, making it fast and cost-effective to construct these graphs.\n",
        "\n",
        "We then implement Lazy GraphRAG using a sequence of LangChain operations:\n",
        "\n",
        "1. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6dFtwI_xmFW"
      },
      "source": [
        "## Colab Environment Setup\n",
        "\n",
        "The following block will configure the environment from the Colab Secrets.\n",
        "To run it, you should have the following Colab Secrets defined and accessible to this notebook:\n",
        "\n",
        "- `OPENAI_API_KEY`: The OpenAI key.\n",
        "- `ASTRA_DB_API_ENDPOINT`: The Astra DB API endpoint.\n",
        "- `ASTRA_DB_APPLICATION_TOKEN`: The Astra DB Application token.\n",
        "- `LANGCHAIN_API_KEY`: Optional. If defined, will enable LangSmith tracing.\n",
        "- `ASTRA_DB_KEYSPACE`: Optional. If defined, will specify the Astra DB keyspace. If not defined, will use the default."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "r0-5VJWGsBM3"
      },
      "outputs": [],
      "source": [
        "# Install modules.\n",
        "%pip install -U langchain-core langchain-astradb langchain-openai langchain-graph-retriever graph-rag-example-helpers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The last package -- `graph-rag-example-helpers` -- includes some helpers for setting up environment helpers and allowing the loading of wikipedia data to be restarted if it fails."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure import paths.\n",
        "import sys\n",
        "import os\n",
        "sys.path.append(\"../../\")\n",
        "\n",
        "# Initialize environment variables.\n",
        "from graph_rag_example_helpers.env import initialize_environment, Environment\n",
        "initialize_environment(Environment.ASTRAPY)\n",
        "\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"lazy-graph-rag\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Loading Data\n",
        "\n",
        "First, we'll demonstrate how to load Wikipedia data into an `AstraDBVectorStore`, using the mentioned articles and keywords as metadata fields.\n",
        "In this section, we're not actually doing anything special for the graph -- we're just populating the metadata with fields that useful describe our content.\n",
        "\n",
        "Specifically:\n",
        "\n",
        "* `id`: Containing the ID of the given article.\n",
        "* `mentions`: Containing a list of IDs mentioned in the article (populate from the content of the article).\n",
        "* `entities`: Containing a list of entities mentioned in the article, computed using the `GLiNEREntityExtractor`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@ Create the AstraDBVectorStore\n",
        "\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_astradb import AstraDBVectorStore\n",
        "\n",
        "COLLECTION = \"lazy_graph_rag\"\n",
        "store = AstraDBVectorStore(\n",
        "    embedding=OpenAIEmbeddings(),\n",
        "    collection_name=COLLECTION,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8u4lD-AqDMMs"
      },
      "outputs": [],
      "source": [
        "#@ Creating Documents from WikiPedia articles\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "import json\n",
        "from typing import Iterator\n",
        "from graph_rag_example_helpers.datasets.wikimultihop import aload_2wikimultihop\n",
        "from langchain_graph_retriever.document_transformers.gliner_entity_extractor import GLiNEREntityExtractor\n",
        "\n",
        "def parse_document(line: str) -> Document:\n",
        "    \"\"\"Reads one JSON line from the wikimultihop dump.\"\"\"\n",
        "    para = json.loads(line)\n",
        "\n",
        "    id = para[\"id\"]\n",
        "\n",
        "    # Use structured information (mentioned Wikipedia IDs) as metadata.\n",
        "    mentioned_ids = [id for m in para[\"mentions\"] for m in m[\"ref_ids\"] or []]\n",
        "\n",
        "    return Document(\n",
        "        id = id,\n",
        "        page_content = \" \".join(para[\"sentences\"]),\n",
        "        metadata = {\n",
        "            \"id\": id,\n",
        "            \"mentions\": mentioned_ids,\n",
        "        }\n",
        "    )\n",
        "\n",
        "GLINER_TRANSFORMER = GLiNEREntityExtractor(\n",
        "    labels = [\"Person\", \"Date\", \"Location\", \"Event\"],\n",
        ")\n",
        "\n",
        "# Load data in batches, using GLiNER to extract entities.\n",
        "def prepare_batch(lines: Iterator[str]) -> Iterator[Document]:\n",
        "    # Parse documents from the batch of lines.\n",
        "    docs = [parse_document(line) for line in lines]\n",
        "\n",
        "    # Apply GLiNER document transformer to extract named entities.\n",
        "    # This seems to cause index out of bounds errors if the sentence is too long.\n",
        "    # docs = GLINER_TRANSFORMER.transform_documents(docs)\n",
        "\n",
        "    return docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@ Loading Data into the Store\n",
        "\n",
        "# Perform the actual loading. This uses a persistence mechanism to\n",
        "# allow re-running if the data isn't completely loaded.\n",
        "#\n",
        "# This takes awhile.\n",
        "# On OS X, it is useful to run with `caffeinate -dis` in a shell,\n",
        "# which prevents the machine from going to sleep and seems to reduce errors.\n",
        "await aload_2wikimultihop(store, prepare_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "At this point, we've created a `VectorStore` with the Wikipedia articles.\n",
        "Each article is associated with metadata identifying other articles it mentions and entities named in the article.\n",
        "This can be used for hybrid search -- performing a vector search for articles similar to a specific question *that also mention a specific term*.\n",
        "\n",
        "In the next section, we'll go a step further and use the mentions and common entities as links between articles, providing a way to navigate and retrieve documents beyond just similarity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Lazy Graph RAG via Hierarchical Summarization\n",
        "\n",
        "As we've noted before, eagerly building a knowledge graph is prohibitively expensive.\n",
        "Microsoft seems to agree, and recently introduced LazyGraphRAG, which enables GraphRAG to be performed late -- after a query is retrieved.\n",
        "\n",
        "We implement the LazyGraphRAG technique using the traversing retrievers as follows:\n",
        "\n",
        "1. Retrieve a good number of nodes using a traversing retrieval.\n",
        "2. Identify communities in the retrieved sub-graph.\n",
        "3. Extract claims from each community relevant to the query using an LLM.\n",
        "4. Rank each of the claims based on the relevance to the question and select the top claims.\n",
        "5. Generate an answer to the question based on the extracted claims."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LangChain for Extracting Claims"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Iterable, List, TypedDict\n",
        "from langchain_openai import ChatOpenAI\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
        "from langchain_core.runnables import chain\n",
        "from operator import itemgetter\n",
        "\n",
        "class Claim(BaseModel):\n",
        "    \"\"\"Representation of an individual claim from a source document(s).\"\"\"\n",
        "    claim: str = Field(description=\"The claim from the original document(s).\")\n",
        "    source_id: str = Field(description=\"Document ID containing the claim.\")\n",
        "\n",
        "\n",
        "class Claims(BaseModel):\n",
        "    \"\"\"Claims extracted from a set of source document(s).\"\"\"\n",
        "    claims: List[Claim] = Field(description=\"The extracted claims.\")\n",
        "\n",
        "MODEL = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "CLAIMS_MODEL = MODEL.with_structured_output(Claims)\n",
        "\n",
        "CLAIMS_PROMPT = ChatPromptTemplate.from_template(\"\"\"\n",
        "Extract claims from the following related documents.\n",
        "\n",
        "Only return claims appearing within the specified documents.\n",
        "If no documents are provided, do not make up claims or documents.\n",
        "\n",
        "Claims (and scores) should be relevant to the question.\n",
        "Don't include claims from the documents if they are not directly or indirectly relevant to the question.\n",
        "\n",
        "If none of the documents make any claims relevant to the question, return an empty list of claims.\n",
        "\n",
        "If multiple documents make similar claims, include the original text of each as separate claims.\n",
        "Score the most useful and authoritative claim higher than similar, lower-quality claims.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "{formatted_documents}\n",
        "\"\"\")\n",
        "\n",
        "# TODO: Few-shot examples? Possibly with a selector?\n",
        "\n",
        "def format_documents_with_ids(documents: Iterable[Document]) -> str:\n",
        "    formatted_docs = \"\\n\\n\".join(\n",
        "        f\"Document ID: {doc.id}\\nContent: {doc.page_content}\"\n",
        "        for doc in documents\n",
        "    )\n",
        "    return formatted_docs\n",
        "\n",
        "CLAIM_CHAIN = (\n",
        "    RunnableParallel({\n",
        "        \"question\": itemgetter(\"question\"),\n",
        "        \"formatted_documents\": itemgetter(\"documents\") | RunnableLambda(format_documents_with_ids),\n",
        "    })\n",
        "   | CLAIMS_PROMPT\n",
        "   | CLAIMS_MODEL\n",
        ")\n",
        "\n",
        "class ClaimsChainInput(TypedDict):\n",
        "    question: str\n",
        "    communities: Iterable[Iterable[Document]]\n",
        "\n",
        "@chain\n",
        "async def claims_chain(input: ClaimsChainInput) -> Iterable[Claim]:\n",
        "    question = input[\"question\"]\n",
        "    communities = input[\"communities\"]\n",
        "\n",
        "    # TODO: Use openai directly so this can use the batch API for performance/cost?\n",
        "    community_claims = await CLAIM_CHAIN.abatch([\n",
        "        { \"question\": question, \"documents\": community } for community in communities\n",
        "    ])\n",
        "    return [claim for community in community_claims for claim in community.claims]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LangChain for Ranking Claims\n",
        "\n",
        "This is based on ideas from [RankRAG](https://arxiv.org/abs/2407.02485).\n",
        "Specifically, the prompt is constructed so that the next token should be `True` if the content is relevant and `False` if not.\n",
        "The probability of the token is used to determine the relevance -- `True` with a higher probability is more relevant than `True` with a lesser probability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "from langchain_core.runnables import chain\n",
        "\n",
        "RANK_PROMPT = ChatPromptTemplate.from_template(\"\"\"\n",
        "Rank the relevance of the following claim to the question.\n",
        "Output \"True\" if the claim is relevant and \"False\" if it is not.\n",
        "Only output True or False.\n",
        "\n",
        "Question: Where is Seattle?\n",
        "\n",
        "Claim: Seattle is in Washington State.\n",
        "\n",
        "Relevant: True\n",
        "\n",
        "Question: Where is LA?\n",
        "\n",
        "Claim: New York City is in New York State.\n",
        "\n",
        "Relevant: False\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Claim: {claim}\n",
        "\n",
        "Relevant:\n",
        "\"\"\")\n",
        "\n",
        "def compute_rank(msg):\n",
        "    logprob = msg.response_metadata[\"logprobs\"][\"content\"][0]\n",
        "    prob = math.exp(logprob[\"logprob\"])\n",
        "    token = logprob[\"token\"]\n",
        "    if token == \"True\":\n",
        "        return prob\n",
        "    elif token == \"False\":\n",
        "        return 1.0 - prob\n",
        "    else:\n",
        "        raise ValueError(f\"Unexpected logprob: {logprob}\")\n",
        "\n",
        "\n",
        "RANK_CHAIN = (\n",
        "    RANK_PROMPT\n",
        "    | MODEL.bind(logprobs=True)\n",
        "    | RunnableLambda(compute_rank)\n",
        ")\n",
        "\n",
        "class RankChainInput(TypedDict):\n",
        "    question: str\n",
        "    claims: Iterable[Claim]\n",
        "\n",
        "@chain\n",
        "async def rank_chain(input: RankChainInput) -> Iterable[Claim]:\n",
        "    # TODO: Use openai directly so this can use the batch API for performance/cost?\n",
        "    claims = input[\"claims\"]\n",
        "    ranks = await RANK_CHAIN.abatch([\n",
        "        { \"question\": input[\"question\"], \"claim\": claim } for claim in claims\n",
        "    ])\n",
        "    rank_claims = sorted(zip(ranks, claims, strict=True),\n",
        "                         key=lambda rank_claim: rank_claim[0])\n",
        "\n",
        "    return [claim for _, claim in rank_claims]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LazyGraphRAG in LangChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Optional, Tuple\n",
        "from langchain_core.retrievers import BaseRetriever\n",
        "from langchain_core.runnables import chain\n",
        "from langchain_graph_retriever.document_graphs import create_graph, group_by_community\n",
        "from langchain_core.language_models import BaseLanguageModel\n",
        "\n",
        "@chain\n",
        "async def lazy_graph_rag(question: str,\n",
        "                         *,\n",
        "                         retriever: BaseRetriever,\n",
        "                         edges: Optional[Iterable[str | Tuple[str, str]]] = None,\n",
        "                         model: BaseLanguageModel,\n",
        "                         max_tokens: int = 1000) -> str:\n",
        "    \"\"\"Retrieve claims relating to the question using LazyGraphRAG.\n",
        "\n",
        "    Returns the top claims up to the given `max_tokens` as a markdown list.\n",
        "    \"\"\"\n",
        "\n",
        "    if edges is None:\n",
        "        try:\n",
        "            edges = retriever.edges\n",
        "        except AttributeError as _:\n",
        "            raise ValueError(f\"Must specify 'edges' or provide a retriever with 'edges' field defined\")\n",
        "\n",
        "    # 1. Retrieve documents using the (traversing) retriever.\n",
        "    documents = await retriever.ainvoke(question)\n",
        "\n",
        "    # 2. Create a graph and extract communities.\n",
        "    documents_by_id, doc_graph = create_graph(\n",
        "        documents,\n",
        "        edges = edges,\n",
        "        directed = False,\n",
        "    )\n",
        "    communities = group_by_community(documents_by_id, doc_graph)\n",
        "\n",
        "    # 3. Extract claims from the communities.\n",
        "    claims = await claims_chain.ainvoke({\"question\": question, \"communities\": communities})\n",
        "\n",
        "    # 4. Rank the claims and select claims up to the given token limit.\n",
        "    result_claims = []\n",
        "    tokens = 0\n",
        "\n",
        "    for claim in await rank_chain.ainvoke({\"question\": question, \"claims\": claims}):\n",
        "        claim_str = f\"- {claim.claim} (Source: {claim.source_id})\"\n",
        "\n",
        "        tokens += model.get_num_tokens(claim_str)\n",
        "        if tokens > max_tokens:\n",
        "            break\n",
        "        result_claims.append(claim_str)\n",
        "\n",
        "    return \"\\n\".join(result_claims)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using LazyGraphRAG in LangChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_community.retrievers.graph_mmr_traversal import GraphMMRTraversalRetriever, AstraMMRTraversalAdapter\n",
        "from langchain_community.retrievers.graph_traversal import GraphTraversalRetriever, AstraTraversalAdapter\n",
        "\n",
        "EDGES = [(\"mentions\", \"id\"), \"entities\"]\n",
        "\n",
        "RETRIEVER = GraphTraversalRetriever(\n",
        "    store = AstraTraversalAdapter(store),\n",
        "    edges = EDGES,\n",
        "    start_k = 100,\n",
        "    depth = 3,\n",
        ")\n",
        "\n",
        "# RETRIEVER = GraphMMRTraversalRetriever(\n",
        "#     store = AstraMMRTraversalAdapter(store),\n",
        "#     edges = EDGES,\n",
        "#     k = 100,\n",
        "#     depth = 5,\n",
        "#     fetch_k = 100,\n",
        "#     adjacent_k = 25,\n",
        "#     lambda_mult = 0.8,\n",
        "#     score_threshold = float(\"-inf\"),\n",
        "# )\n",
        "\n",
        "ANSWER_PROMPT = PromptTemplate.from_template(\"\"\"\n",
        "Answer the question based on the supporting claims.\n",
        "\n",
        "Only use information from the claims. Do not guess or make up any information.\n",
        "\n",
        "Where possible, reference and quote the supporting claims.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Claims:\n",
        "{claims}\n",
        "\"\"\")\n",
        "\n",
        "LAZY_GRAPH_RAG_CHAIN = (\n",
        "    {  \"question\": RunnablePassthrough(),\n",
        "       \"claims\": RunnablePassthrough() |\n",
        "           lazy_graph_rag.bind(\n",
        "               retriever=RETRIEVER,\n",
        "               model=MODEL,\n",
        "               max_tokens=1000,\n",
        "           )\n",
        "    }\n",
        "    | ANSWER_PROMPT\n",
        "    | MODEL\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Claims: ['- Azerbaijani statements assert that the current Armenian territory was under the rule of various Turkic tribes, empires, and khanates until the Treaty of Turkmenchay after the Russo-Persian War of 1826–1828. (Source: 29018074)', '- The government of Azerbaijan claims that the territory of the modern Armenian republic were lands that once belonged to Azerbaijanis. (Source: 29018074)', '- Azerbaijan has many islands along the coast of the Caspian Sea. (Source: 29321112)', '- Azerbaijan is associated with the Nagorno-Karabakh region. (Source: 12725998)', \"- Ilham Aliyev, the current president of Azerbaijan, has stated that the territory of Armenia is a part of 'ancient Turk and Azerbaijani land.' (Source: 29018074)\", '- Azerbaijan is nearly surrounded by mountains, with the Greater Caucasus range to the northeast along the border with Russia, the Lesser Caucasus range to the west along the border with Armenia, and the Talysh Mountains to the south along the border with Iran. (Source: 54204858)', '- The exclave of Nakhchivan is bounded by Armenia to the north and east, Iran to the south and west, while having a short borderline with Turkey to the northwest. (Source: 22602331)', \"- West Azerbaijan Province is separated from Armenia by Turkey's short border with the Azerbaijan Republic. (Source: 47567329)\", '- Baku is the capital of Azerbaijan. (Source: 54156046)', \"- Historically, the name 'Azerbaijan' referred to the region south of the Aras River, now known as Iranian Azerbaijan in northwestern Iran. (Source: 60746553)\", '- Azerbaijan is located in Baku, Azerbaijan. (Source: 34637440)', '- Azerbaijan shares a short international border with Turkey at the southeastern tip of the Iğdır Province on the Turkish side and at the northwestern tip of the Nakhchivan Autonomous Republic on the Azerbaijani side. (Source: 54722186)', '- Azerbaijan is located at specific extreme points that are farther north, south, east, or west than any other location in the country. (Source: 6922486)', '- The exclave of Nakhchivan in Azerbaijan is bounded by Armenia to the north and east, Iran to the south and west, and has a long border with Turkey in the northwest. (Source: 746)', \"- West Azerbaijan Province in Iran borders Azerbaijan's Nakhchivan Autonomous Republic. (Source: 47567329)\", '- A small part of Nakhchivan, an autonomous republic of Azerbaijan, also borders Turkey to the northwest. (Source: 1082)', '- Azerbaijan is a region in northwestern Iran. (Source: 55114239)', '- Baku is the capital of Azerbaijan. (Source: 57440731)', '- Qazaxbərəsi is a village in the municipality of Qaçaqkənd in the Neftchala Rayon of Azerbaijan. (Source: 20167956)', '- Azerbaijan is the largest country in the Caucasus region of Eurasia. (Source: 16278429)', '- The exclave of Nakhichevan in Azerbaijan is bounded by Armenia to the north and east, Iran to the south and west, and has a short borderline with Turkey to the northwest. (Source: 8916407)', '- Baku is a city in Azerbaijan. (Source: 45628876)', '- Azerbaijan borders the Caspian Sea to the east, Georgia and Russia to the north, Iran to the south, and Armenia to the southwest and west. (Source: 1082)', '- Əyrək is a village in the Lachin Rayon of Azerbaijan. (Source: 20330980)', '- Azerbaijan is a landlocked country in the South Caucasus region of Eurasia at the crossroads of Eastern Europe and Western Asia. (Source: 746)', '- Azerbaijan is a country. (Source: 59918837)', '- Əzizabad is a village and municipality in the Masally Rayon of Azerbaijan. (Source: 18926248)', '- Azerbaijan State Pantomime Theatre is located in Baku, Azerbaijan. (Source: 35042351)', '- Tazakend is a village in the Bilasuvar Rayon of Azerbaijan. (Source: 18845936)', '- Qazıdərə is a village in the Lachin Rayon of Azerbaijan. (Source: 18868187)', '- Ajinohur is a lake of Azerbaijan. (Source: 28461434)', '- Bashsyz is a village in the Jalilabad Rayon of Azerbaijan. (Source: 18846480)']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Azerbaijan is a country located in the Caucasus region of Eurasia. It is bordered by the Caspian Sea to the east, Georgia and Russia to the north, Iran to the south, and Armenia to the southwest and west. Additionally, Azerbaijan has a short international border with Turkey at the southeastern tip of the Iğdır Province on the Turkish side and at the northwestern tip of the Nakhchivan Autonomous Republic on the Azerbaijani side. Baku is the capital city of Azerbaijan. (Sources: 16278429, 1082, 54722186, 54156046)', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 126, 'prompt_tokens': 1023, 'total_tokens': 1149, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_d28bcae782', 'finish_reason': 'stop', 'logprobs': None}, id='run-ea6b25e0-540d-43a0-978f-1756c176da1c-0', usage_metadata={'input_tokens': 1023, 'output_tokens': 126, 'total_tokens': 1149, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "await LAZY_GRAPH_RAG_CHAIN.ainvoke(\"Where is Azerbaijan?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The LazyGraphRAG chain is great when a question needs to consider a large amount of relevant information in order to produce a thorough answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "This post introduced _traversing retrievers_ which allow any `VectorStore` to be traversed as a knowledge graph based on properties in the metadata.\n",
        "This means you can focus on populating and using your `VectorStore` with useful metadata and add GraphRAG when you need it.\n",
        "We also saw that these traversing retrievers mean that any `VectorStore` can be used with LazyGraphRAG, without needing to change the stored documents.\n",
        "\n",
        "Knowledge Graphs and GraphRAG shouldn't be hard or scary.\n",
        "Start simple and easily overlay edges when you need them.\n",
        "\n",
        "These traversing retrievers and LazyGraphRAG summarization work well with agents.\n",
        "You can create tools that use different retriever configurations, for instance, searching for articles \"near\" existing articles or distinguishing between questions that only need a few references and deeper questions which need to retrieve and summarize a larger amount of content.\n",
        "We'll show how to combine these graph techniques with agents in future posts.\n",
        "Until then, ..."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
