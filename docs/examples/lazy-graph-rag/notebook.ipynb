{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqhMgAGwmLXc"
      },
      "source": [
        "# Retrieval Beyond Similarity: LazyGraphRAG in LangChain\n",
        "\n",
        "## Introduction\n",
        "\n",
        "We originally [created the `GraphVectorStore`](https://www.datastax.com/blog/knowledge-graphs-for-rag-without-a-graphdb) as a way of efficiently handling structured relationships between nodes.\n",
        "We [introduced this to LangChain](https://www.datastax.com/blog/now-in-langchain-graph-vector-store-add-structured-data-to-rag-apps), and [demonstrated significant cost savings](https://hackernoon.com/how-to-save-$70k-building-a-knowledge-graph-for-rag-on-6m-wikipedia-pages) by lazily traversing the metadata instead of eagerly generating the knowledge graph. \n",
        "The benefit of lazily traversing nodes based on extracted metadata has since been supported by [Microsoft’s introduction of LazyGraphRAG](https://www.microsoft.com/en-us/research/blog/lazygraphrag-setting-a-new-standard-for-quality-and-cost/).\n",
        "\n",
        "In this post, we demonstrate two exciting refinements:\n",
        "\n",
        "1. Replacing the GraphVectorStore with traversing Retrievers. This allows any existing LangChain VectorStore to be used as a graph by lazily traversing metadata fields. This allows an already populated VectorStore to be traversed without making any changes, and really delivers on the promise of lazy GraphRAG – only creating the graph when you perform retrieval.\n",
        "\n",
        "2. An implementation of LazyGraphRAG performing clustering and summarization on top of any LangChain VectorStore providing metadata filtering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6dFtwI_xmFW"
      },
      "source": [
        "## Colab Environment Setup\n",
        "\n",
        "The following block will configure the environment from the Colab Secrets.\n",
        "To run it, you should have the following Colab Secrets defined and accessible to this notebook:\n",
        "\n",
        "- `OPENAI_API_KEY`: The OpenAI key.\n",
        "- `ASTRA_DB_API_ENDPOINT`: The Astra DB API endpoint.\n",
        "- `ASTRA_DB_APPLICATION_TOKEN`: The Astra DB Application token.\n",
        "- `LANGCHAIN_API_KEY`: Optional. If defined, will enable LangSmith tracing.\n",
        "- `ASTRA_DB_KEYSPACE`: Optional. If defined, will specify the Astra DB keyspace. If not defined, will use the default."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "r0-5VJWGsBM3"
      },
      "outputs": [],
      "source": [
        "# Install modules.\n",
        "%pip install -U -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "2o4XjqFHyFE_",
        "outputId": "ba45c715-2a0a-411e-c812-2e790d056e91"
      },
      "outputs": [],
      "source": [
        "# Override LangChain from Eric's fork.\n",
        "# Won't be needed once this is available.\n",
        "%pip install --force-reinstall git+https://github.com/epinzur/langchain.git@graph_retrievers#subdirectory=libs/community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure import paths.\n",
        "import sys\n",
        "import os\n",
        "sys.path.append(\"../../\")\n",
        "\n",
        "# Initialize environment variables.\n",
        "from utils import initialize_environment, Environment\n",
        "initialize_environment(Environment.ASTRAPY)\n",
        "\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"2025_LazyGraphRAG\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@ Create the AstraDBVectorStore\n",
        "\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_astradb import AstraDBVectorStore\n",
        "\n",
        "COLLECTION = \"lazy_graph_rag\"\n",
        "store = AstraDBVectorStore(\n",
        "    embedding=OpenAIEmbeddings(),\n",
        "    collection_name=COLLECTION,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 0: Loading Data\n",
        "\n",
        "In this section, we demonstrate how to load Wikipedia data into an `AstraDBVectorStore`, using the mentioned articles and entities extracted using [GLiNER](https://github.com/urchade/GLiNER) as fields.\n",
        "This populates the following metadata fields:\n",
        "\n",
        "* `id`: Containing the ID of the given article.\n",
        "* `mentions`: Containing a list of IDs mentioned in the article (populate from the content of the article).\n",
        "* `entities`: Containing a list of entities mentioned in the article, computed using the `GLiNEREntityExtractor`.\n",
        "\n",
        "This only needs to be done once to populate the datastore.\n",
        "At this point, we're just setting metadata on the documents -- in the next part we'll see how to traverse these metadata fields during retrieval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8u4lD-AqDMMs"
      },
      "outputs": [],
      "source": [
        "#@ Creating Documents from WikiPedia articles\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "import json\n",
        "from typing import Iterator\n",
        "from datasets.wikimultihop.load import aload_2wikimultihop\n",
        "from langchain_community.document_transformers.gliner_entitiy_extractor import GLiNEREntityExtractor\n",
        "\n",
        "def parse_document(line: str) -> Document:\n",
        "    \"\"\"Reads one JSON line from the wikimultihop dump.\"\"\"\n",
        "    para = json.loads(line)\n",
        "\n",
        "    id = para[\"id\"]\n",
        "\n",
        "    # Use structured information (mentioned Wikipedia IDs) as metadata.\n",
        "    mentioned_ids = [id for m in para[\"mentions\"] for m in m[\"ref_ids\"] or []]\n",
        "\n",
        "    return Document(\n",
        "        id = id,\n",
        "        page_content = \" \".join(para[\"sentences\"]),\n",
        "        metadata = {\n",
        "            \"id\": id,\n",
        "            \"mentions\": mentioned_ids,\n",
        "        }\n",
        "    )\n",
        "\n",
        "GLINER_TRANSFORMER = GLiNEREntityExtractor(\n",
        "    labels = [\"Person\", \"Date\", \"Location\", \"Event\"],\n",
        ")\n",
        "\n",
        "# Load data in batches, using GLiNER to extract entities.\n",
        "def prepare_batch(lines: Iterator[str]) -> Iterator[Document]:\n",
        "    # Parse documents from the batch of lines.\n",
        "    docs = [parse_document(line) for line in lines]\n",
        "\n",
        "    # Apply GLiNER document transformer to extract named entities.\n",
        "    # This seems to cause index out of bounds errors if the sentence is too long.\n",
        "    # docs = GLINER_TRANSFORMER.transform_documents(docs)\n",
        "\n",
        "    return docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@ Loading Data into the Store\n",
        "\n",
        "# Perform the actual loading. This uses a persistence mechanism to\n",
        "# allow re-running if the data isn't completely loaded.\n",
        "#\n",
        "# This takes awhile.\n",
        "# On OS X, it is useful to run with `caffeinate -dis` in a shell,\n",
        "# which prevents the machine from going to sleep and seems to reduce errors.\n",
        "await aload_2wikimultihop(store, prepare_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "At this point, we've created a `VectorStore` with the Wikipedia articles.\n",
        "Each article is associated with metadata identifying other articles it mentions and entities named in the article.\n",
        "This can be used for hybrid search -- performing a vector search for articles similar to a specific question *that also mention a specific term*.\n",
        "\n",
        "In the next section, we'll go a step further and use the mentions and common entities as links between articles, providing a way to navigate and retrieve documents beyond just similarity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Traversing Retrievers\n",
        "\n",
        "The traversing retrievers allow any `VectorStore` that supports metadata filtering to be used as a graph.\n",
        "We just need to declare which metadata fields correspond to edges, and the retriever takes care of the rest.\n",
        "\n",
        "In our case, we have two kinds of edges:\n",
        "1. We connect an article to mentioned articles by linking `mentions` to `id`.\n",
        "2. We connect an article to other articles on the same entity by linking `entities` to `entities`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GraphTraversalRetriever\n",
        "\n",
        "The basic `GraphTraversalRetriever` starts with `start_k` nodes most similar to the query, and then retrieves all edges to a given depth.\n",
        "It is particularly useful for retrieving a sub-graph around the question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.retrievers.graph_traversal import GraphTraversalRetriever, AstraTraversalAdapter\n",
        "\n",
        "traversal = GraphTraversalRetriever(\n",
        "    store = AstraTraversalAdapter(store),\n",
        "    edges = [(\"mentions\", \"id\"), \"entities\"],\n",
        "    # These are optional, with the shown defaults\n",
        "    start_k = 4,\n",
        "    depth = 4,\n",
        ")\n",
        "traversal.invoke(\"where is Lithuania?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GraphMMRTraversalRetriever\n",
        "With higher depths, retrieving the entire sub-graph may be equivalent to retrieving the whole graph.\n",
        "In these cases, the MMR traversal allows retrieving a fixed number of nodes using an MMR scoring algorithm to choose which nodes to traverse while balancing between similarity and diversity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.retrievers.graph_mmr_traversal import GraphMMRTraversalRetriever, AstraMMRTraversalAdapter\n",
        "mmr_traversal = GraphMMRTraversalRetriever(\n",
        "    store = AstraMMRTraversalAdapter(store),\n",
        "    edges = [(\"mentions\", \"id\"), \"entities\"],\n",
        "    # These are optional, with the shown defaults\n",
        "    k = 4,\n",
        "    depth = 2,\n",
        "    fetch_k = 100,\n",
        "    adjacent_k = 10,\n",
        "    lambda_mult = 0.5,\n",
        "    score_threshold = float(\"-inf\"),\n",
        ")\n",
        "mmr_traversal.invoke(\"where is Lithuania?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One nice thing about the links being defined on top of metadata fields is that the same content can be treated as a graph in different ways depending on the situation.\n",
        "For example, in some cases you may only want to follow the explicit `mentions` to `id` links, and you can do that by just not including the `entities` links."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Lazy Graph RAG via Hierarchical Summarization\n",
        "\n",
        "As we've noted before, eagerly building a knowledge graph is prohibitively expensive.\n",
        "Microsoft seems to agree, and recently introduced LazyGraphRAG, which enables GraphRAG to be performed late -- after a query is retrieved.\n",
        "\n",
        "We implement the LazyGraphRAG technique using the traversing retrievers as follows:\n",
        "\n",
        "1. Retrieve a good number of nodes using a traversing retrieval.\n",
        "2. Identify communities in the retrieved sub-graph.\n",
        "3. Extract claims from each community relevant to the query using an LLM.\n",
        "4. Rank each of the claims based on the relevance to the question and select the top claims.\n",
        "5. Generate an answer to the question based on the extracted claims."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LangChain for Extracting Claims"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Iterable, List, TypedDict\n",
        "from langchain_openai import ChatOpenAI\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
        "from langchain_core.runnables import chain\n",
        "from operator import itemgetter\n",
        "\n",
        "class Claim(BaseModel):\n",
        "    \"\"\"Representation of an individual claim from a source document(s).\"\"\"\n",
        "    claim: str = Field(description=\"The claim from the original document(s).\")\n",
        "    source_id: str = Field(description=\"Document ID containing the claim.\")\n",
        "\n",
        "\n",
        "class Claims(BaseModel):\n",
        "    \"\"\"Claims extracted from a set of source document(s).\"\"\"\n",
        "    claims: List[Claim] = Field(description=\"The extracted claims.\")\n",
        "\n",
        "MODEL = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "CLAIMS_MODEL = MODEL.with_structured_output(Claims)\n",
        "\n",
        "CLAIMS_PROMPT = ChatPromptTemplate.from_template(\"\"\"\n",
        "Extract claims from the following related documents.\n",
        "\n",
        "Only return claims appearing within the specified documents.\n",
        "If no documents are provided, do not make up claims or documents.\n",
        "\n",
        "Claims (and scores) should be relevant to the question.\n",
        "Don't include claims from the documents if they are not directly or indirectly relevant to the question.\n",
        "\n",
        "If none of the documents make any claims relevant to the question, return an empty list of claims.\n",
        "\n",
        "If multiple documents make similar claims, include the original text of each as separate claims.\n",
        "Score the most useful and authoritative claim higher than similar, lower-quality claims.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "{formatted_documents}\n",
        "\"\"\")\n",
        "\n",
        "# TODO: Few-shot examples? Possibly with a selector?\n",
        "\n",
        "def format_documents_with_ids(documents: Iterable[Document]) -> str:\n",
        "    formatted_docs = \"\\n\\n\".join(\n",
        "        f\"Document ID: {doc.id}\\nContent: {doc.page_content}\"\n",
        "        for doc in documents\n",
        "    )\n",
        "    return formatted_docs\n",
        "\n",
        "CLAIM_CHAIN = (\n",
        "    RunnableParallel({\n",
        "        \"question\": itemgetter(\"question\"),\n",
        "        \"formatted_documents\": itemgetter(\"documents\") | RunnableLambda(format_documents_with_ids),\n",
        "    })\n",
        "   | CLAIMS_PROMPT\n",
        "   | CLAIMS_MODEL\n",
        ")\n",
        "\n",
        "class ClaimsChainInput(TypedDict):\n",
        "    question: str\n",
        "    communities: Iterable[Iterable[Document]]\n",
        "\n",
        "@chain\n",
        "async def claims_chain(input: ClaimsChainInput) -> Iterable[Claim]:\n",
        "    question = input[\"question\"]\n",
        "    communities = input[\"communities\"]\n",
        "\n",
        "    # TODO: Use openai directly so this can use the batch API for performance/cost?\n",
        "    community_claims = await CLAIM_CHAIN.abatch([\n",
        "        { \"question\": question, \"documents\": community } for community in communities\n",
        "    ])\n",
        "    return [claim for community in community_claims for claim in community.claims]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LangChain for Ranking Claims\n",
        "\n",
        "This is based on ideas from [RankRAG](https://arxiv.org/abs/2407.02485).\n",
        "Specifically, the prompt is constructed so that the next token should be `True` if the content is relevant and `False` if not.\n",
        "The probability of the token is used to determine the relevance -- `True` with a higher probability is more relevant than `True` with a lesser probability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "from langchain_core.runnables import chain\n",
        "\n",
        "RANK_PROMPT = ChatPromptTemplate.from_template(\"\"\"\n",
        "Rank the relevance of the following claim to the question.\n",
        "Output \"True\" if the claim is relevant and \"False\" if it is not.\n",
        "Only output True or False.\n",
        "\n",
        "Question: Where is Seattle?\n",
        "\n",
        "Claim: Seattle is in Washington State.\n",
        "\n",
        "Relevant: True\n",
        "\n",
        "Question: Where is LA?\n",
        "\n",
        "Claim: New York City is in New York State.\n",
        "\n",
        "Relevant: False\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Claim: {claim}\n",
        "\n",
        "Relevant:\n",
        "\"\"\")\n",
        "\n",
        "def compute_rank(msg):\n",
        "    logprob = msg.response_metadata[\"logprobs\"][\"content\"][0]\n",
        "    prob = math.exp(logprob[\"logprob\"])\n",
        "    token = logprob[\"token\"]\n",
        "    if token == \"True\":\n",
        "        return prob\n",
        "    elif token == \"False\":\n",
        "        return 1.0 - prob\n",
        "    else:\n",
        "        raise ValueError(f\"Unexpected logprob: {logprob}\")\n",
        "\n",
        "\n",
        "RANK_CHAIN = (\n",
        "    RANK_PROMPT\n",
        "    | MODEL.bind(logprobs=True)\n",
        "    | RunnableLambda(compute_rank)\n",
        ")\n",
        "\n",
        "class RankChainInput(TypedDict):\n",
        "    question: str\n",
        "    claims: Iterable[Claim]\n",
        "\n",
        "@chain\n",
        "async def rank_chain(input: RankChainInput) -> Iterable[Claim]:\n",
        "    # TODO: Use openai directly so this can use the batch API for performance/cost?\n",
        "    claims = input[\"claims\"]\n",
        "    ranks = await RANK_CHAIN.abatch([\n",
        "        { \"question\": input[\"question\"], \"claim\": claim } for claim in claims\n",
        "    ])\n",
        "    rank_claims = sorted(zip(ranks, claims, strict=True),\n",
        "                         key=lambda rank_claim: rank_claim[0])\n",
        "\n",
        "    return [claim for _, claim in rank_claims]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LazyGraphRAG in LangChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Optional, Tuple\n",
        "from langchain_core.retrievers import BaseRetriever\n",
        "from langchain_core.runnables import chain\n",
        "from lazy_graph_rag import create_graph, group_by_community\n",
        "from langchain_core.language_models import BaseLanguageModel\n",
        "\n",
        "@chain\n",
        "async def lazy_graph_rag(question: str,\n",
        "                         *,\n",
        "                         retriever: BaseRetriever,\n",
        "                         edges: Optional[Iterable[str | Tuple[str, str]]] = None,\n",
        "                         model: BaseLanguageModel,\n",
        "                         max_tokens: int = 1000) -> str:\n",
        "    \"\"\"Retrieve claims relating to the question using LazyGraphRAG.\n",
        "\n",
        "    Returns the top claims up to the given `max_tokens` as a markdown list.\n",
        "    \"\"\"\n",
        "\n",
        "    if edges is None:\n",
        "        try:\n",
        "            edges = retriever.edges\n",
        "        except AttributeError as _:\n",
        "            raise ValueError(f\"Must specify 'edges' or provide a retriever with 'edges' field defined\")\n",
        "\n",
        "    # 1. Retrieve documents using the (traversing) retriever.\n",
        "    documents = await retriever.ainvoke(question)\n",
        "\n",
        "    # 2. Create a graph and extract communities.\n",
        "    documents_by_id, doc_graph = create_graph(\n",
        "        documents,\n",
        "        edges = edges,\n",
        "        directed = False,\n",
        "    )\n",
        "    communities = group_by_community(documents_by_id, doc_graph)\n",
        "\n",
        "    # 3. Extract claims from the communities.\n",
        "    claims = await claims_chain.ainvoke({\"question\": question, \"communities\": communities})\n",
        "\n",
        "    # 4. Rank the claims and select claims up to the given token limit.\n",
        "    result_claims = []\n",
        "    tokens = 0\n",
        "\n",
        "    for claim in await rank_chain.ainvoke({\"question\": question, \"claims\": claims}):\n",
        "        claim_str = f\"- {claim.claim} (Source: {claim.source_id})\"\n",
        "\n",
        "        tokens += model.get_num_tokens(claim_str)\n",
        "        if tokens > max_tokens:\n",
        "            break\n",
        "        result_claims.append(claim_str)\n",
        "\n",
        "    return \"\\n\".join(result_claims)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using LazyGraphRAG in LangChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_community.retrievers.graph_mmr_traversal import GraphMMRTraversalRetriever, AstraMMRTraversalAdapter\n",
        "from langchain_community.retrievers.graph_traversal import GraphTraversalRetriever, AstraTraversalAdapter\n",
        "\n",
        "EDGES = [(\"mentions\", \"id\"), \"entities\"]\n",
        "\n",
        "RETRIEVER = GraphTraversalRetriever(\n",
        "    store = AstraTraversalAdapter(store),\n",
        "    edges = EDGES,\n",
        "    start_k = 100,\n",
        "    depth = 3,\n",
        ")\n",
        "\n",
        "# RETRIEVER = GraphMMRTraversalRetriever(\n",
        "#     store = AstraMMRTraversalAdapter(store),\n",
        "#     edges = EDGES,\n",
        "#     k = 100,\n",
        "#     depth = 5,\n",
        "#     fetch_k = 100,\n",
        "#     adjacent_k = 25,\n",
        "#     lambda_mult = 0.8,\n",
        "#     score_threshold = float(\"-inf\"),\n",
        "# )\n",
        "\n",
        "ANSWER_PROMPT = PromptTemplate.from_template(\"\"\"\n",
        "Answer the question based on the supporting claims.\n",
        "\n",
        "Only use information from the claims. Do not guess or make up any information.\n",
        "\n",
        "Where possible, reference and quote the supporting claims.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Claims:\n",
        "{claims}\n",
        "\"\"\")\n",
        "\n",
        "LAZY_GRAPH_RAG_CHAIN = (\n",
        "    {  \"question\": RunnablePassthrough(),\n",
        "       \"claims\": RunnablePassthrough() |\n",
        "           lazy_graph_rag.bind(\n",
        "               retriever=RETRIEVER,\n",
        "               model=MODEL,\n",
        "               max_tokens=1000,\n",
        "           )\n",
        "    }\n",
        "    | ANSWER_PROMPT\n",
        "    | MODEL\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Claims: ['- Azerbaijani statements assert that the current Armenian territory was under the rule of various Turkic tribes, empires, and khanates until the Treaty of Turkmenchay after the Russo-Persian War of 1826–1828. (Source: 29018074)', '- The government of Azerbaijan claims that the territory of the modern Armenian republic were lands that once belonged to Azerbaijanis. (Source: 29018074)', '- Azerbaijan has many islands along the coast of the Caspian Sea. (Source: 29321112)', '- Azerbaijan is associated with the Nagorno-Karabakh region. (Source: 12725998)', \"- Ilham Aliyev, the current president of Azerbaijan, has stated that the territory of Armenia is a part of 'ancient Turk and Azerbaijani land.' (Source: 29018074)\", '- Azerbaijan is nearly surrounded by mountains, with the Greater Caucasus range to the northeast along the border with Russia, the Lesser Caucasus range to the west along the border with Armenia, and the Talysh Mountains to the south along the border with Iran. (Source: 54204858)', '- The exclave of Nakhchivan is bounded by Armenia to the north and east, Iran to the south and west, while having a short borderline with Turkey to the northwest. (Source: 22602331)', \"- West Azerbaijan Province is separated from Armenia by Turkey's short border with the Azerbaijan Republic. (Source: 47567329)\", '- Baku is the capital of Azerbaijan. (Source: 54156046)', \"- Historically, the name 'Azerbaijan' referred to the region south of the Aras River, now known as Iranian Azerbaijan in northwestern Iran. (Source: 60746553)\", '- Azerbaijan is located in Baku, Azerbaijan. (Source: 34637440)', '- Azerbaijan shares a short international border with Turkey at the southeastern tip of the Iğdır Province on the Turkish side and at the northwestern tip of the Nakhchivan Autonomous Republic on the Azerbaijani side. (Source: 54722186)', '- Azerbaijan is located at specific extreme points that are farther north, south, east, or west than any other location in the country. (Source: 6922486)', '- The exclave of Nakhchivan in Azerbaijan is bounded by Armenia to the north and east, Iran to the south and west, and has a long border with Turkey in the northwest. (Source: 746)', \"- West Azerbaijan Province in Iran borders Azerbaijan's Nakhchivan Autonomous Republic. (Source: 47567329)\", '- A small part of Nakhchivan, an autonomous republic of Azerbaijan, also borders Turkey to the northwest. (Source: 1082)', '- Azerbaijan is a region in northwestern Iran. (Source: 55114239)', '- Baku is the capital of Azerbaijan. (Source: 57440731)', '- Qazaxbərəsi is a village in the municipality of Qaçaqkənd in the Neftchala Rayon of Azerbaijan. (Source: 20167956)', '- Azerbaijan is the largest country in the Caucasus region of Eurasia. (Source: 16278429)', '- The exclave of Nakhichevan in Azerbaijan is bounded by Armenia to the north and east, Iran to the south and west, and has a short borderline with Turkey to the northwest. (Source: 8916407)', '- Baku is a city in Azerbaijan. (Source: 45628876)', '- Azerbaijan borders the Caspian Sea to the east, Georgia and Russia to the north, Iran to the south, and Armenia to the southwest and west. (Source: 1082)', '- Əyrək is a village in the Lachin Rayon of Azerbaijan. (Source: 20330980)', '- Azerbaijan is a landlocked country in the South Caucasus region of Eurasia at the crossroads of Eastern Europe and Western Asia. (Source: 746)', '- Azerbaijan is a country. (Source: 59918837)', '- Əzizabad is a village and municipality in the Masally Rayon of Azerbaijan. (Source: 18926248)', '- Azerbaijan State Pantomime Theatre is located in Baku, Azerbaijan. (Source: 35042351)', '- Tazakend is a village in the Bilasuvar Rayon of Azerbaijan. (Source: 18845936)', '- Qazıdərə is a village in the Lachin Rayon of Azerbaijan. (Source: 18868187)', '- Ajinohur is a lake of Azerbaijan. (Source: 28461434)', '- Bashsyz is a village in the Jalilabad Rayon of Azerbaijan. (Source: 18846480)']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Azerbaijan is a country located in the Caucasus region of Eurasia. It is bordered by the Caspian Sea to the east, Georgia and Russia to the north, Iran to the south, and Armenia to the southwest and west. Additionally, Azerbaijan has a short international border with Turkey at the southeastern tip of the Iğdır Province on the Turkish side and at the northwestern tip of the Nakhchivan Autonomous Republic on the Azerbaijani side. Baku is the capital city of Azerbaijan. (Sources: 16278429, 1082, 54722186, 54156046)', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 126, 'prompt_tokens': 1023, 'total_tokens': 1149, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_d28bcae782', 'finish_reason': 'stop', 'logprobs': None}, id='run-ea6b25e0-540d-43a0-978f-1756c176da1c-0', usage_metadata={'input_tokens': 1023, 'output_tokens': 126, 'total_tokens': 1149, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "await LAZY_GRAPH_RAG_CHAIN.ainvoke(\"Where is Azerbaijan?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The LazyGraphRAG chain is great when a question needs to consider a large amount of relevant information in order to produce a thorough answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "This post introduced _traversing retrievers_ which allow any `VectorStore` to be traversed as a knowledge graph based on properties in the metadata.\n",
        "This means you can focus on populating and using your `VectorStore` with useful metadata and add GraphRAG when you need it.\n",
        "We also saw that these traversing retrievers mean that any `VectorStore` can be used with LazyGraphRAG, without needing to change the stored documents.\n",
        "\n",
        "Knowledge Graphs and GraphRAG shouldn't be hard or scary.\n",
        "Start simple and easily overlay edges when you need them.\n",
        "\n",
        "These traversing retrievers and LazyGraphRAG summarization work well with agents.\n",
        "You can create tools that use different retriever configurations, for instance, searching for articles \"near\" existing articles or distinguishing between questions that only need a few references and deeper questions which need to retrieve and summarize a larger amount of content.\n",
        "We'll show how to combine these graph techniques with agents in future posts.\n",
        "Until then, ..."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
